{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":105804,"databundleVersionId":12825033,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:28.514968Z","iopub.execute_input":"2025-06-25T14:54:28.515598Z","iopub.status.idle":"2025-06-25T14:54:28.519453Z","shell.execute_reply.started":"2025-06-25T14:54:28.515577Z","shell.execute_reply":"2025-06-25T14:54:28.518730Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def apply_fast_filter(img, pattern):\n    \"\"\"Применяет 2x2 фильтр к изображению.\"\"\"\n    # Создаём новое изображение с теми же размерами\n    filtered_img = np.zeros_like(img)\n\n    # Каждому пикселю в блоке 2x2 оставляем только нужный канал\n    # Верхний левый\n    filtered_img[0::2, 0::2, pattern[0]] = img[0::2, 0::2, pattern[0]]\n    # Верхний правый\n    filtered_img[0::2, 1::2, pattern[1]] = img[0::2, 1::2, pattern[1]]\n    # Нижний левый\n    filtered_img[1::2, 0::2, pattern[2]] = img[1::2, 0::2, pattern[2]]\n    # Нижний правый\n    filtered_img[1::2, 1::2, pattern[3]] = img[1::2, 1::2, pattern[3]]\n\n    return filtered_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:28.520496Z","iopub.execute_input":"2025-06-25T14:54:28.520734Z","iopub.status.idle":"2025-06-25T14:54:28.541877Z","shell.execute_reply.started":"2025-06-25T14:54:28.520717Z","shell.execute_reply":"2025-06-25T14:54:28.541362Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"patterns = [\n    [0,1,2,0],\n    [0,2,1,0],\n    [1,0,0,2],\n    [2,0,0,1],\n    [1,0,2,1],\n    [1,2,0,1],\n    [0,1,1,2],\n    [2,1,1,0],\n    [2,0,1,2],\n    [2,1,0,2],\n    [0,2,2,1],\n    [1,2,2,0],\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:28.542477Z","iopub.execute_input":"2025-06-25T14:54:28.542646Z","iopub.status.idle":"2025-06-25T14:54:28.566619Z","shell.execute_reply.started":"2025-06-25T14:54:28.542634Z","shell.execute_reply":"2025-06-25T14:54:28.565861Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\n\ndef apply_fast_filter(img_np, pattern):\n    \"\"\"Применяет 2x2 фильтр к изображению numpy.\"\"\"\n    filtered_img = np.zeros_like(img_np)\n    filtered_img[0::2, 0::2, pattern[0]] = img_np[0::2, 0::2, pattern[0]]\n    filtered_img[0::2, 1::2, pattern[1]] = img_np[0::2, 1::2, pattern[1]]\n    filtered_img[1::2, 0::2, pattern[2]] = img_np[1::2, 0::2, pattern[2]]\n    filtered_img[1::2, 1::2, pattern[3]] = img_np[1::2, 1::2, pattern[3]]\n    return filtered_img\n\ndef apply_filter_func(pil_img, filter_id):\n    pattern = patterns[filter_id]\n    img_np = np.array(pil_img)  # PIL -> numpy (H, W, 3)\n    filtered_np = apply_fast_filter(img_np, pattern)\n    # Можно вернуть PIL для совместимости с transform\n    return Image.fromarray(filtered_np)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:28.567850Z","iopub.execute_input":"2025-06-25T14:54:28.568070Z","iopub.status.idle":"2025-06-25T14:54:28.584453Z","shell.execute_reply.started":"2025-06-25T14:54:28.568056Z","shell.execute_reply":"2025-06-25T14:54:28.583776Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nclass FilteredRestoreDataset(Dataset):\n    def __init__(self, original_dir, filter_num=12, transform=None, apply_filter_func=None):\n        self.original_dir = original_dir\n        self.transform = transform\n        self.filter_num = filter_num\n        self.apply_filter_func = apply_filter_func\n        \n        self.img_names = sorted(os.listdir(original_dir))\n        \n    def __len__(self):\n        return len(self.img_names) * self.filter_num\n    \n    def __getitem__(self, idx):\n        # idx - индекс по всем фильтрованным версиям всех картинок\n        img_idx = idx // self.filter_num  # индекс исходной картинки\n        filter_id = idx % self.filter_num  # какой фильтр применяем\n        \n        img_name = self.img_names[img_idx]\n        img_path = os.path.join(self.original_dir, img_name)\n        img = Image.open(img_path).convert('RGB')\n        \n        # Применяем фильтр\n        filtered_img = self.apply_filter_func(img, filter_id)\n        \n        if self.transform:\n            filtered_img = self.transform(filtered_img)\n            original_img = self.transform(img)\n        else:\n            # По умолчанию - преобразуем в тензор\n            to_tensor = transforms.ToTensor()\n            filtered_img = to_tensor(filtered_img)\n            original_img = to_tensor(img)\n        \n        return filtered_img, original_img, filter_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:28.585211Z","iopub.execute_input":"2025-06-25T14:54:28.585459Z","iopub.status.idle":"2025-06-25T14:54:28.613526Z","shell.execute_reply.started":"2025-06-25T14:54:28.585439Z","shell.execute_reply":"2025-06-25T14:54:28.612850Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from torchvision import transforms\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import train_test_split\n\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\ndataset = FilteredRestoreDataset(\n    '/kaggle/input/tst-day-1/train/train/real_images', \n    filter_num=12, \n    transform=transform, \n    apply_filter_func=apply_filter_func\n)\n\n# Получаем индексы оригинальных изображений\nall_img_idxs = list(range(len(dataset.img_names)))\n\n# Делим на train/val (например, 80/20)\ntrain_img_idxs, val_img_idxs = train_test_split(all_img_idxs, test_size=0.2, random_state=42, shuffle=True)\n\n# Функция для получения индексов по фильтрам\ndef create_filtered_indices(img_idxs, filter_num):\n    indices = []\n    for img_idx in img_idxs:\n        for filter_id in range(filter_num):\n            idx = img_idx * filter_num + filter_id\n            indices.append(idx)\n    return indices\n\ntrain_indices = create_filtered_indices(train_img_idxs, dataset.filter_num)\nval_indices = create_filtered_indices(val_img_idxs, dataset.filter_num)\n\n# Создаем Subset датасеты\ntrain_dataset = Subset(dataset, train_indices)\nval_dataset = Subset(dataset, val_indices)\n\n# Создаем загрузчики\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:28.614342Z","iopub.execute_input":"2025-06-25T14:54:28.614836Z","iopub.status.idle":"2025-06-25T14:54:28.652427Z","shell.execute_reply.started":"2025-06-25T14:54:28.614821Z","shell.execute_reply":"2025-06-25T14:54:28.651780Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# SE-блок (Squeeze-and-Excitation)\nclass SEBlock(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, channels // reduction, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        scale = self.fc(x)\n        return x * scale\n\n# Простой Residual блок с SE\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_ch)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_ch)\n        self.se = SEBlock(out_ch)\n        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n\n    def forward(self, x):\n        identity = self.skip(x)\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.se(out)\n        out += identity\n        out = self.relu(out)\n        return out\n\n# Мощный U-Net без ResNet\nclass CustomUNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, filter_size=12):\n        super().__init__()\n\n        # Этапы Энкодера\n        self.encoder1 = ResidualBlock(in_channels, 64)  # (B, 64, H/2, W/2)\n        self.encoder2 = ResidualBlock(64, 128)  # (B, 128, H/4, W/4)\n        self.encoder3 = ResidualBlock(128, 256)  # (B, 256, H/8, W/8)\n        self.encoder4 = ResidualBlock(256, 512)  # (B, 512, H/16, W/16)\n\n        # Линейный слой для преобразования one-hot представления фильтра\n        self.filter_fc = nn.Linear(filter_size, 256)  # Фильтр преобразуется в 256-мерный вектор\n\n        # Этапы Декодера\n        self.up4 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n        self.dec4 = ResidualBlock(256 + 256, 256)\n\n        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.dec3 = ResidualBlock(128 + 128, 128)\n\n        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n        self.dec2 = ResidualBlock(64 + 64, 64)\n\n        self.up1 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n        self.dec1 = ResidualBlock(64 + 64, 64)\n\n        # Финальная свертка для сегментации\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x, filter_one_hot):\n        # Преобразуем one-hot фильтр в вектор признаков\n        filter_features = self.filter_fc(filter_one_hot)  # (B, 256)\n        \n        # Этапы энкодера\n        x1 = self.encoder1(x)  # (B, 64, H/2, W/2)\n        x2 = self.encoder2(x1)  # (B, 128, H/4, W/4)\n        x3 = self.encoder3(x2)  # (B, 256, H/8, W/8)\n        x4 = self.encoder4(x3)  # (B, 512, H/16, W/16)\n        \n        # Этапы декодера\n        d4 = self.up4(x4)  # (B, 256, H/8, W/8)\n        filter_d4 = filter_features.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, d4.size(2), d4.size(3))  # Расширяем filter_features\n        d4 = torch.cat([d4, x3, filter_d4], dim=1)  # (B, 256+256, H/8, W/8)\n        d4 = self.dec4(d4)\n    \n        d3 = self.up3(d4)  # (B, 128, H/4, W/4)\n        filter_d3 = filter_features.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, d3.size(2), d3.size(3))  # Расширяем filter_features\n        d3 = torch.cat([d3, x2, filter_d3], dim=1)  # (B, 128+128, H/4, W/4)\n        d3 = self.dec3(d3)\n    \n        d2 = self.up2(d3)  # (B, 64, H/2, W/2)\n        filter_d2 = filter_features.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, d2.size(2), d2.size(3))  # Расширяем filter_features\n        d2 = torch.cat([d2, x1, filter_d2], dim=1)\n        d2 = self.dec2(d2)\n    \n        d1 = self.up1(d2)  # (B, 64, H, W)\n        filter_d1 = filter_features.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, d1.size(2), d1.size(3))  # Расширяем filter_features\n        d1 = torch.cat([d1, x, filter_d1], dim=1)\n        d1 = self.dec1(d1)\n    \n        out = self.final_conv(d1)  # (B, out_channels, H, W)\n        return torch.sigmoid(out)  # Используем сигмоиду для получения вероятности для сегментации\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:28.654347Z","iopub.execute_input":"2025-06-25T14:54:28.654583Z","iopub.status.idle":"2025-06-25T14:54:28.668893Z","shell.execute_reply.started":"2025-06-25T14:54:28.654565Z","shell.execute_reply":"2025-06-25T14:54:28.668383Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n\nclass CombinedLossNoPretrained(nn.Module):\n    def __init__(self, device, alpha=0.7, beta=0.3):\n        super().__init__()\n        self.alpha = alpha  # вес MSE\n        self.beta = beta    # вес SSIM\n        self.device = device\n        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)  # Инициализация метрики SSIM\n        \n    def forward(self, pred, target):\n        # Вычисляем MSE\n        mse_loss = F.mse_loss(pred, target)\n        \n        # Вычисляем SSIM\n        ssim_loss = 1 - self.ssim(pred, target)  # SSIM хочет минимизировать 1 - SSIM\n        \n        # Комбинированный лосс\n        total_loss = self.alpha * mse_loss + self.beta * ssim_loss\n        return total_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:28.669404Z","iopub.execute_input":"2025-06-25T14:54:28.669570Z","iopub.status.idle":"2025-06-25T14:54:28.694191Z","shell.execute_reply.started":"2025-06-25T14:54:28.669557Z","shell.execute_reply":"2025-06-25T14:54:28.693699Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch.optim as optim\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Инициализация модели (например, для 3-канальных изображений на входе и 1 канала на выходе)\nmodel = CustomUNet(in_channels=3, out_channels=1).to(device)\n\ncriterion = CombinedLossNoPretrained(device)  # Простой MSE лосс для начала\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\nnum_epochs = 20\nnum_filters = 12  # Кол-во фильтров\n\nfor epoch in range(num_epochs):\n    # --- Обучение ---\n    model.train()\n    running_loss = 0.0\n    \n    for filtered_imgs, original_imgs, filter_ids in train_loader:\n        filtered_imgs = filtered_imgs.to(device)\n        original_imgs = original_imgs.to(device)\n        filter_ids = filter_ids.to(device)\n\n        filter_one_hot = torch.zeros(filtered_imgs.size(0), num_filters).to(device)\n        filter_one_hot.scatter_(1, filter_ids.view(-1, 1), 1)\n\n        optimizer.zero_grad()\n        outputs = model(filtered_imgs, filter_one_hot)\n        loss = criterion(outputs, original_imgs)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * filtered_imgs.size(0)\n\n    epoch_train_loss = running_loss / len(train_loader.dataset)\n\n    # --- Валидация ---\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for filtered_imgs, original_imgs, filter_ids in val_loader:\n            filtered_imgs = filtered_imgs.to(device)\n            original_imgs = original_imgs.to(device)\n            filter_ids = filter_ids.to(device)\n\n            filter_one_hot = torch.zeros(filtered_imgs.size(0), num_filters).to(device)\n            filter_one_hot.scatter_(1, filter_ids.view(-1, 1), 1)\n\n            outputs = model(filtered_imgs, filter_one_hot)\n            loss = criterion(outputs, original_imgs)\n            val_loss += loss.item() * filtered_imgs.size(0)\n\n    epoch_val_loss = val_loss / len(val_loader.dataset)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:28.694793Z","iopub.execute_input":"2025-06-25T14:54:28.695025Z","iopub.status.idle":"2025-06-25T14:54:30.177035Z","shell.execute_reply.started":"2025-06-25T14:54:28.695003Z","shell.execute_reply":"2025-06-25T14:54:30.175982Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3795586486.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1332247866.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, filter_one_hot)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Этапы декодера\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0md4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, 256, H/8, W/8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mfilter_d4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Расширяем filter_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0md4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_d4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, 256+256, H/8, W/8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.39 GiB is free. Process 4062 has 13.34 GiB memory in use. Of the allocated memory 9.44 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.39 GiB is free. Process 4062 has 13.34 GiB memory in use. Of the allocated memory 9.44 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"def detect_filter(filtered_img_np, patterns):\n    \"\"\"\n    Определяет, какой фильтр был применён, сравнивая отфильтрованное изображение с результатами\n    применения каждого фильтра (паттерна) к тому же изображению.\n    \n    :param filtered_img_np: отфильтрованное изображение (numpy array)\n    :param patterns: список паттернов фильтров (список numpy array паттернов)\n    \n    :return: индекс фильтра, который применен к изображению (или -1, если фильтр не найден)\n    \"\"\"\n    for i, pattern in enumerate(patterns):\n        # Применяем паттерн к самому себе (фильтрация на исходном изображении)\n        filtered_test = apply_fast_filter(filtered_img_np, pattern)\n        \n        # Сравниваем результат фильтрации с самим собой\n        if np.array_equal(filtered_test, filtered_img_np):\n            return i  # Индекс найденного фильтра\n    \n    return -1  # Если фильтр не найден\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:30.177567Z","iopub.status.idle":"2025-06-25T14:54:30.177851Z","shell.execute_reply.started":"2025-06-25T14:54:30.177736Z","shell.execute_reply":"2025-06-25T14:54:30.177748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(model, filtered_tensor, filter_one_hot, device):\n    \"\"\"\n    Выполняет инференс модели для отфильтрованного изображения и возвращает восстановленное изображение.\n\n    :param model: модель нейронной сети (например, ResNet34UNetWithFilter)\n    :param filtered_tensor: отфильтрованное изображение в виде тензора (torch.Tensor), размер (C,H,W)\n    :param filter_one_hot: one-hot вектор фильтра (torch.Tensor), размер (num_filters,)\n    :param device: устройство (cuda или cpu)\n\n    :return: восстановленное изображение (PIL Image)\n    \"\"\"\n    model.eval()\n\n    filtered_tensor = filtered_tensor.to(device)\n    filter_one_hot = torch.tensor(filter_one_hot).to(device)\n\n    with torch.no_grad():\n        output = model(filtered_tensor.unsqueeze(0), filter_one_hot.unsqueeze(0))\n\n    output_img_np = output.squeeze(0).cpu().numpy().transpose(1, 2, 0)  # (H, W, C)\n    output_img_np = (output_img_np * 255).clip(0, 255).astype(np.uint8)\n\n    output_img = Image.fromarray(output_img_np)\n\n    return output_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:30.179383Z","iopub.status.idle":"2025-06-25T14:54:30.179707Z","shell.execute_reply.started":"2025-06-25T14:54:30.179514Z","shell.execute_reply":"2025-06-25T14:54:30.179525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass TestDataset(Dataset):\n    def __init__(self, test_dir, transform=None):\n        self.test_dir = test_dir\n        self.img_names = sorted(os.listdir(test_dir))\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.img_names)\n    \n    def __getitem__(self, idx):\n        img_name = self.img_names[idx]\n        img_path = os.path.join(self.test_dir, img_name)\n        img = Image.open(img_path).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img, img_name\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:30.180627Z","iopub.status.idle":"2025-06-25T14:54:30.180902Z","shell.execute_reply.started":"2025-06-25T14:54:30.180788Z","shell.execute_reply":"2025-06-25T14:54:30.180801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_transform = transforms.Compose([\n    transforms.Resize((128,128)),\n    transforms.ToTensor(),   # обязательно преобразуем в тензор\n])\n\n\n\ntest_dataset = TestDataset('/kaggle/input/tst-day-1/test/test/filtered_images', transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:30.182461Z","iopub.status.idle":"2025-06-25T14:54:30.182750Z","shell.execute_reply.started":"2025-06-25T14:54:30.182598Z","shell.execute_reply":"2025-06-25T14:54:30.182610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\n# Загружаем sample submission\nsample = pd.read_csv('/kaggle/input/tst-day-1/sample_submission.csv')\n\n# Создадим словарь для быстрых вставок\nrestored_np_dict = {}\n\nmodel.eval()\nfor filtered_img, img_name in test_loader:\n    filtered_img = filtered_img[0]  # batch=1, берём PIL\n    img_id = os.path.splitext(img_name[0])[0]\n\n    restored_img, _ = inference(model, filtered_img, patterns, device)\n\n    restored_np = np.array(restored_img).reshape(-1)\n    restored_np_dict[img_id] = restored_np\n\n# Заполняем sample_submission\nfor idx, row in sample.iterrows():\n    img_id = row['id']\n    if str(img_id) in restored_np_dict:\n        sample.loc[idx, sample.columns[1:]] = restored_np_dict[img_id]\n    else:\n        print(f\"Warning: {img_id} not found in predictions!\")\n\n# Сохраняем итоговый submission\nprint(\"Submission saved to submission.csv\")\n","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:30.183814Z","iopub.status.idle":"2025-06-25T14:54:30.184079Z","shell.execute_reply.started":"2025-06-25T14:54:30.183977Z","shell.execute_reply":"2025-06-25T14:54:30.183987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n\nnum_pixels = 128 * 128  # Размер блока для первых и последних пикселей\n\n# Преобразуем данные в массивы пикселей\nfor idx, row in sample.iterrows():\n    # Берем все пиксели после первого столбца\n    sample_pixels = row[1:].values  # Пиксели, начиная с 1-го столбца\n    \n    # Разбиваем на два блока по 128*128 пикселей\n    first_part = sample_pixels[:num_pixels]\n    last_part = sample_pixels[-num_pixels:]\n    \n    # Поменяли местами\n    sample_pixels[:num_pixels] = last_part\n    sample_pixels[-num_pixels:] = first_part\n    \n    # Заполняем в sample\n    sample.loc[idx, sample.columns[1:]] = sample_pixels\n\n# Сохраняем итоговый submission\nsample.to_csv('submission.csv', index=False)\nprint(\"Submission with swapped pixels saved to submission_swapped.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:54:30.184702Z","iopub.status.idle":"2025-06-25T14:54:30.184949Z","shell.execute_reply.started":"2025-06-25T14:54:30.184835Z","shell.execute_reply":"2025-06-25T14:54:30.184847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Whoever is watching it, know that I am screwed up(((","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}